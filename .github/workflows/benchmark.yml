name: Performance Benchmarking

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    # Run daily at 3 AM UTC
    - cron: '0 3 * * *'
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: mcp-server/package-lock.json
      
      - name: Install dependencies
        working-directory: ./mcp-server
        run: |
          npm ci
          npm install -D autocannon clinic
      
      - name: Build server
        working-directory: ./mcp-server
        run: npm run build --if-present || npx tsc
      
      - name: Start MCP server
        working-directory: ./mcp-server
        run: |
          # Start server in background
          npm start &
          SERVER_PID=$!
          echo "SERVER_PID=$SERVER_PID" >> $GITHUB_ENV
          
          # Wait for server to be ready
          echo "Waiting for server to start..."
          for i in {1..30}; do
            if curl -f http://localhost:3000/health 2>/dev/null; then
              echo "Server is ready"
              break
            fi
            sleep 1
          done
      
      - name: Run response time benchmark
        id: response_time
        run: |
          # Create benchmark script
          cat << 'EOF' > benchmark.js
          const autocannon = require('autocannon');
          
          async function runBenchmark() {
            const result = await autocannon({
              url: 'http://localhost:3000',
              connections: 10,
              pipelining: 1,
              duration: 30,
              requests: [
                {
                  method: 'POST',
                  path: '/api/execute',
                  headers: {
                    'content-type': 'application/json'
                  },
                  body: JSON.stringify({
                    command: 'test',
                    params: {}
                  })
                }
              ]
            });
            
            console.log('Response Time Results:');
            console.log(`- Avg: ${result.latency.mean}ms`);
            console.log(`- Min: ${result.latency.min}ms`);
            console.log(`- Max: ${result.latency.max}ms`);
            console.log(`- P99: ${result.latency.p99}ms`);
            console.log(`- Requests/sec: ${result.requests.average}`);
            
            // Check if response time is under 100ms
            if (result.latency.mean > 100) {
              console.error(`FAIL: Average response time ${result.latency.mean}ms exceeds 100ms target`);
              process.exit(1);
            }
            
            // Output for GitHub
            console.log(`::set-output name=avg_response::${result.latency.mean}`);
            console.log(`::set-output name=p99_response::${result.latency.p99}`);
            console.log(`::set-output name=rps::${result.requests.average}`);
          }
          
          runBenchmark().catch(console.error);
          EOF
          
          node benchmark.js
      
      - name: Memory usage monitoring
        id: memory
        run: |
          # Monitor memory for 30 seconds
          echo "Monitoring memory usage..."
          
          MAX_MEM=0
          for i in {1..30}; do
            if [ -n "$SERVER_PID" ]; then
              MEM=$(ps -o rss= -p $SERVER_PID | awk '{print $1/1024}')
              if (( $(echo "$MEM > $MAX_MEM" | bc -l) )); then
                MAX_MEM=$MEM
              fi
            fi
            sleep 1
          done
          
          echo "Max memory usage: ${MAX_MEM}MB"
          echo "max_memory=${MAX_MEM}" >> $GITHUB_OUTPUT
          
          # Check if memory is under 256MB
          if (( $(echo "$MAX_MEM > 256" | bc -l) )); then
            echo "FAIL: Memory usage ${MAX_MEM}MB exceeds 256MB target"
            exit 1
          fi
      
      - name: CPU usage profiling
        id: cpu
        run: |
          # Profile CPU usage
          echo "Profiling CPU usage..."
          
          if [ -n "$SERVER_PID" ]; then
            # Get average CPU over 10 seconds
            AVG_CPU=$(top -b -n 10 -d 1 -p $SERVER_PID | grep $SERVER_PID | awk '{sum+=$9; count++} END {print sum/count}')
            echo "Average CPU usage: ${AVG_CPU}%"
            echo "avg_cpu=${AVG_CPU}" >> $GITHUB_OUTPUT
          fi
      
      - name: Load testing
        run: |
          # Stress test with higher load
          cat << 'EOF' > loadtest.js
          const autocannon = require('autocannon');
          
          async function loadTest() {
            console.log('Running load test...');
            const result = await autocannon({
              url: 'http://localhost:3000',
              connections: 100,
              pipelining: 10,
              duration: 60
            });
            
            console.log('Load Test Results:');
            console.log(`- Total requests: ${result.requests.total}`);
            console.log(`- Errors: ${result.errors}`);
            console.log(`- Timeouts: ${result.timeouts}`);
            console.log(`- Non-2xx responses: ${result.non2xx}`);
            
            if (result.errors > 0 || result.timeouts > 0) {
              console.error('Load test failed with errors or timeouts');
              process.exit(1);
            }
          }
          
          loadTest().catch(console.error);
          EOF
          
          node loadtest.js
      
      - name: Stop server
        if: always()
        run: |
          if [ -n "$SERVER_PID" ]; then
            kill $SERVER_PID || true
          fi
      
      - name: Compare with baseline
        id: compare
        run: |
          # Load baseline if exists
          BASELINE_FILE=".github/performance-baseline.json"
          
          if [ -f "$BASELINE_FILE" ]; then
            echo "Comparing with baseline..."
            
            BASELINE_RESPONSE=$(jq -r '.response_time' $BASELINE_FILE)
            BASELINE_MEMORY=$(jq -r '.memory' $BASELINE_FILE)
            
            CURRENT_RESPONSE="${{ steps.response_time.outputs.avg_response }}"
            CURRENT_MEMORY="${{ steps.memory.outputs.max_memory }}"
            
            # Calculate percentage change
            RESPONSE_CHANGE=$(echo "scale=2; (($CURRENT_RESPONSE - $BASELINE_RESPONSE) / $BASELINE_RESPONSE) * 100" | bc)
            MEMORY_CHANGE=$(echo "scale=2; (($CURRENT_MEMORY - $BASELINE_MEMORY) / $BASELINE_MEMORY) * 100" | bc)
            
            echo "Response time change: ${RESPONSE_CHANGE}%"
            echo "Memory change: ${MEMORY_CHANGE}%"
            
            # Fail if performance degrades by more than 10%
            if (( $(echo "$RESPONSE_CHANGE > 10" | bc -l) )); then
              echo "FAIL: Response time degraded by ${RESPONSE_CHANGE}%"
              exit 1
            fi
            
            if (( $(echo "$MEMORY_CHANGE > 10" | bc -l) )); then
              echo "FAIL: Memory usage increased by ${MEMORY_CHANGE}%"
              exit 1
            fi
          else
            echo "No baseline found, skipping comparison"
          fi
      
      - name: Generate report
        if: always()
        run: |
          cat << EOF >> $GITHUB_STEP_SUMMARY
          # Performance Benchmark Results
          
          ## Response Time
          - Average: ${{ steps.response_time.outputs.avg_response }}ms (Target: <100ms)
          - P99: ${{ steps.response_time.outputs.p99_response }}ms
          - Requests/sec: ${{ steps.response_time.outputs.rps }}
          
          ## Resource Usage
          - Max Memory: ${{ steps.memory.outputs.max_memory }}MB (Target: <256MB)
          - Avg CPU: ${{ steps.cpu.outputs.avg_cpu }}%
          
          ## Status
          EOF
          
          if [ "${{ steps.response_time.outputs.avg_response }}" -lt "100" ] && [ "${{ steps.memory.outputs.max_memory }}" -lt "256" ]; then
            echo "✅ All performance targets met" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Performance targets not met" >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const comment = `## 📊 Performance Benchmark Results
            
            | Metric | Value | Target | Status |
            |--------|-------|--------|--------|
            | Avg Response Time | ${{ steps.response_time.outputs.avg_response }}ms | <100ms | ${{ steps.response_time.outputs.avg_response < 100 && '✅' || '❌' }} |
            | P99 Response Time | ${{ steps.response_time.outputs.p99_response }}ms | - | - |
            | Max Memory | ${{ steps.memory.outputs.max_memory }}MB | <256MB | ${{ steps.memory.outputs.max_memory < 256 && '✅' || '❌' }} |
            | Avg CPU | ${{ steps.cpu.outputs.avg_cpu }}% | - | - |
            | Requests/sec | ${{ steps.response_time.outputs.rps }} | - | - |
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
      
      - name: Update baseline
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          # Update baseline for future comparisons
          cat << EOF > .github/performance-baseline.json
          {
            "response_time": ${{ steps.response_time.outputs.avg_response }},
            "memory": ${{ steps.memory.outputs.max_memory }},
            "cpu": ${{ steps.cpu.outputs.avg_cpu }},
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
          EOF
          
          # This would normally be committed back to the repo
          echo "Baseline updated (would be committed in production)"